<!DOCTYPE html>
<html lang="en" class="bg-neutral-900 text-neutral-100">
<head>
  <meta charset="UTF-8" />
  <title>Multi-Intervention Sequential Decision Modeling</title>
  <link rel="icon" type="image/png" sizes="48x48" href="/images/favicon-48x48.png">
  <link rel="icon" type="image/png" sizes="192x192" href="/images/favicon-192x192.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="stylesheet" href="/styles/output.css?v=1771525456">
</head>
<body class="bg-neutral-900 text-neutral-100 font-sans">
  <header class="max-w-3xl mx-auto px-6 py-8 flex justify-between items-center">
    <a href="/" class="text-2xl font-bold text-neutral-100 hover:text-white transition">
      Kyran Flynn
    </a>
    <nav class="space-x-6 text-neutral-400">
      <a href="/" class="hover:text-white transition">Portfolio</a>
      <a href="/about/" class="hover:text-white transition">About</a>
    </nav>
  </header>

  <main class="max-w-3xl mx-auto px-6 pt-4 pb-2 text-center">
    
  <div class="max-w-3xl mx-auto px-6 pt-4 pb-4 text-neutral-100 text-left">
    <a href="/" class="inline-block mb-6 text-neutral-400 hover:text-neutral-200 transition">
      ← Back to Portfolio
    </a>

    <h1 class="text-3xl font-bold mb-4">Multi-Intervention Sequential Decision Modeling</h1>

    <div class="prose prose-invert max-w-none">
      <div class="meta">
<p><strong>Overview</strong> — I built an end-to-end experimental pipeline to study reinforcement learning in sequential decision settings where multiple interacting interventions must be selected in combination at each timestep. The pipeline integrates synthetic data generation, model training, controlled experimentation, and evaluation. At its core is a graph-structured reinforcement learning architecture that produces intervention-level value estimates, enabling scalable decision-making without exhaustively evaluating exponentially many intervention combinations.</p>
<p><strong>Setting</strong> — Columbia <a href="https://ieor.columbia.edu/">IEOR</a> PhD program, advised by <a href="https://lily-x.github.io/">Prof. Lily Xu</a>.</p>
<p><strong>Github</strong> — <a href="https://github.com/KyFlynn/graph-structured-rl">graph-structured-rl</a>.</p>
<p><strong>Language</strong> — Python.</p>
<p><strong>Tech Stack</strong> — <a href="https://pytorch.org/">PyTorch</a>, <a href="https://pyg.org/">PyTorch Geometric</a>, <a href="https://gymnasium.farama.org/index.html">Gymnasium</a>, <a href="https://optuna.org/">Optuna</a>, <a href="https://networkx.org/">NetworkX</a>, <a href="https://scikit-learn.org/stable/">scikit-learn</a>, <a href="https://www.gurobi.com">Gurobi</a>.</p>
</div>
<div class="card">
  <h2>Sequential Decision Model</h2>
<p>I built an end-to-end experimental pipeline to improve how reinforcement learning agents make decisions when multiple interventions must be chosen in combination at each timestep. The motivation came from clinical-style decision settings where at each step, a combination of treatments (e.g., drug type, intensity, frequency) must be selected, often with limited data.</p>
<p>Standard reinforcement learning struggles in these environments. As the number of intervention options grows, the action space explodes combinatorially, making learning unstable and sample-inefficient.</p>
<p>We can visualize the intervention options and the state variables as nodes in a graph.</p>
<p><img src="/images/graph-rl-drawing-1.jpg" alt="graph-rl-drawing-1"></p>
<p>The core idea was to exploit structure. First, we estimate a dependency graph between interventions and system state variables using regression-based methods. This produced an approximate map of how individual interventions influence the system.</p>
<p><img src="/images/graph-rl-drawing-2.jpg" alt="graph-rl-drawing-2"></p>
<p>We then embed this structure into a graph neural network-based Q-learning architecture. Rather than evaluating every possible intervention combination, the model incrementally constructs an action by estimating the marginal value of individual interventions conditioned on the current state and prior selections.</p>
<p><img src="/images/graph-rl-drawing-3.jpg" alt="graph-rl-drawing-3"></p>
<p>We repeat this process until a choice has been made for all interventions, and this concludes decision making for one timestep.</p>
<p>This reframing avoids exhaustive combination evaluation, and allows each partial decision to incorporate structured information about system dynamics. To support this, I built a full experimental pipeline: synthetic data generation, model training, and controlled experimentation to evaluate performance against the standard approach.</p>
</div>
<div class="card">
  <h2>Presentation</h2>
<p>Department presentation of this project:</p>
  <div class="mt-4">
    <iframe
      width="100%"
      height="420"
      src="https://www.youtube.com/embed/M3XRnSD5bx0"
      title="Multi-Intervention Decision Modeling Presentation"
      frameborder="0"
      allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
      referrerpolicy="strict-origin-when-cross-origin"
      loading="lazy"
      allowfullscreen
    ></iframe>
  </div>
</div>

    </div>
  </div>

  </main>

  <footer class="max-w-3xl mx-auto px-6 py-10 mt-0 text-sm text-gray-500 text-center">
    © <script>document.write(new Date().getFullYear())</script> Kyran Flynn
  </footer>
</body>
</html>
