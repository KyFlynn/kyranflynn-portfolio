<!DOCTYPE html>
<html lang="en" class="bg-neutral-900 text-neutral-100">
<head>
  <meta charset="UTF-8" />
  <title>Causal Structure-Informed Reinforcement Learning</title>
  <link rel="stylesheet" href="/styles/output.css">
</head>
<body class="bg-neutral-900 text-neutral-100 font-sans">
  <header class="max-w-3xl mx-auto px-6 py-8 flex justify-between items-center">
    <a href="/" class="text-2xl font-bold text-neutral-100 hover:text-white transition">
      Kyran Flynn
    </a>
    <nav class="space-x-6 text-neutral-400">
      <a href="/" class="hover:text-white transition">Portfolio</a>
      <a href="/about/" class="hover:text-white transition">About</a>
    </nav>
  </header>

  <main class=" max-w-3xl mx-auto px-6 pt-4 pb-2 text-center">
    
  <div class="max-w-3xl mx-auto px-6 pt-4 pb-4 text-neutral-100 text-left">
    <a href="/" class="inline-block mb-6 text-neutral-400 hover:text-neutral-200 transition">
      ← Back to Portfolio
    </a>

    <h1 class="text-3xl font-bold mb-4">Causal Structure-Informed Reinforcement Learning</h1>

    <div class="prose prose-invert max-w-none">
      <div class="meta">
<p><strong>Key Idea</strong> — Built graph neural network-based reinforcement learning models leveraging causal structure for improved data-efficiency in multi-intervention environments.</p>
<p><strong>Program</strong> — <a href="https://ieor.columbia.edu/">Columbia IEOR</a>, advised by <a href="https://lily-x.github.io/">Prof. Lily Xu</a>.</p>
<p><strong>Tech Stack</strong> — <a href="https://pytorch.org/">PyTorch</a>, <a href="https://pyg.org/">PyG</a>, <a href="https://www.gurobi.com/faqs/gurobipy/">GurobiPy</a>, <a href="https://www.gurobi.com/features/gurobi-machine-learning/">Gurobi-ML</a>, <a href="https://www.pywhy.org/">PyWhy</a>, <a href="https://networkx.org/">NetworkX</a>.</p>
</div>
<div class="card"><h2>Overview</h2>
<p>We experimented with graph neural network (GNN)–based reinforcement learning (RL) frameworks to improve model data-efficiency in data-limited environments with multiple intervention options. Standard RL algorithms often overlook the causal relationships between interventions and system states, so we focused on learning them and leveraging the resulting graph structure using GNNs. Our goal was to explore whether this approach could reduce data needs, and scale effectively to high-dimensional settings.</p>
</div>
<div class="card"><h2>Implementation</h2>
<ul>
<li>Developed model architectures, synthetic problems, and testing framework from scratch</li>
<li>Each intervention (sub-action) and state variable is represented as a node in a bipartite graph</li>
<li>A graph neural network (GNN) propagates information between state and sub-action nodes to estimate Q-values jointly</li>
<li>The combinatorial optimization step for selecting the best intervention set is solved via Gurobi using the learned Q-value function</li>
<li>This approach enables scalable training and decision-making in environments with large binary sub-action spaces</li>
</ul>
</div>
<div class="card"><h2>Highlights</h2>
<ul>
<li>Integrated GNN message passing into a custom Q-learning loop for multi-action reinforcement learning.</li>
<li>Designed a modular PyTorch Geometric training pipeline with Gurobi-based optimization for policy selection.</li>
<li>Demonstrated improved learning stability and faster convergence on synthetic multi-intervention testbeds.</li>
<li>Applied causal inference tools from the PyWhy framework to improve interpretability of learned dependencies.</li>
</ul>
</div>

    </div>
  </div>

  </main>

  <footer class="max-w-3xl mx-auto px-6 py-10 mt-0 text-sm text-gray-500 text-center">
    © 2025 Kyran Flynn
  </footer>
</body>
</html>